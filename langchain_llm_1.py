# -*- coding: utf-8 -*-
"""langchain_llm_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O-JssUBk5TA3DG71GycbpEM9bNfw8j8t

# Import Libraries
"""

import os
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
from transformers import pipeline
from langchain.docstore.document import Document
from transformers import AutoTokenizer, pipeline

"""# Open the file "app.py" in read mode"""

with open("/content/app.py") as file:
   code_text = file.read()

"""# Initialize a text splitter to break the code into smaller chunks"""

splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,
    chunk_overlap=50,
    separators=["\nclass ", "\ndef ", "\n", " "]
)
documents = splitter.create_documents([code_text])

"""# Define the embedding model to be used"""

embedding_model_name = "BAAI/bge-small-en-v1.5"
embedding = HuggingFaceEmbeddings(model_name=embedding_model_name)

"""# Create a FAISS vector store from the split documents"""

vectorstore = FAISS.from_documents(documents, embedding)

"""# Create a FAISS vector store"""

db = FAISS.from_documents(documents, embedding)

"""# Define the Hugging Face model name"""

model_name =  "tiiuae/falcon-rw-1b"
tokenizer = AutoTokenizer.from_pretrained(model_name)

"""# Create a Hugging Face text-generation pipeline using Falcon"""

llm_pipeline = pipeline(
    "text-generation",
    model=model_name,
    tokenizer=tokenizer,
    device="cpu",
    max_new_tokens=64,
    temperature=0.7,
    pad_token_id=tokenizer.eos_token_id
)

"""# Wrap the Hugging Face pipeline in a LangChain-compatible LLM"""

llm = HuggingFacePipeline(pipeline=llm_pipeline)

"""# Build a RetrievalQA chain using the Falcon LLM and FAISS retriever"""

qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=vectorstore.as_retriever())

"""# Define the user query"""

question = "What are the key functions or classes in this file?"
result = qa_chain.invoke({"query": question})

print(" Question:", question)
print(" Answer:", result['result'])

"""Final Remarks and Conclusion

In this project, we successfully implemented a Retrieval-Augmented Generation (RAG) workflow to analyze and understand Python code files. The workflow combines a FAISS vector store for semantic search over code chunks with the Falcon-RW-1B LLM for generating human-readable explanations. The embedding model efficiently converts code into vector representations, enabling precise retrieval of relevant sections, while the LLM interprets and summarizes these sections to answer queries about functions, classes, and overall structure.

This approach demonstrates how modern AI techniques can be leveraged for automated code comprehension, making it easier to review, document, and understand large codebases. The modular design allows for scaling to multiple files or projects, and the integration of retrieval with generation ensures that responses are both accurate and contextually relevant. Overall, the project highlights the power of combining embeddings, vector databases, and large language models to enhance developer productivity and code analysis workflows.
"""