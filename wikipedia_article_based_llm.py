# -*- coding: utf-8 -*-
"""wikipedia_Article_basedLLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Adv_9ahsMYrS_qKFYftmhlvzvJ7Ak6WZ

# Import Libraries
**bold text**
"""

from langchain.document_loaders import WikipediaLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM

"""# Initialize the Wikipedia loader with a query"""

loader = WikipediaLoader(query="indian constitution",load_max_docs=30)
docs = loader.load()

"""# Initialize a text splitter to break documents into smaller chunks"""

splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)
chunks = splitter.split_documents(docs)

"""# Initialize the embedding model"""

embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

"""# Create a FAISS vector store from the loaded documents using the embedding model"""

vectorstore = FAISS.from_documents(docs, embedding_model)

retriever = vectorstore.as_retriever()

"""# Load a pre-trained LLM from Hugging Face (Phi-2 model)"""

model_id = "microsoft/phi-2"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", torch_dtype="auto")

generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

"""# Define the user query"""

query ="indian constitution history??"
print(f"Retrieving chunks for: {query}")

retriever = vectorstore.as_retriever()
relevant_docs = retriever.get_relevant_documents(query)

# Concatenate the content of retrieved documents
context = "\n".join([doc.page_content for doc in relevant_docs])

print("\nRetrieved Context:\n")
print(context)

"""Conclusion

In this project, we implemented a retrieval-augmented generative AI pipeline to analyze Wikipedia content. Using a FAISS vector store to semantically index documents about the “Indian Constitution” and a sentence-transformer embedding model to encode text chunks, the system can efficiently retrieve relevant information. The Phi-2 LLM then interprets the retrieved context and generates human-readable answers to queries.

This workflow demonstrates how embedding-based retrieval combined with generative AI enables accurate and context-aware knowledge extraction from large text corpora, providing an effective tool for research, education, and automated information retrieval.
"""